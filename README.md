# Ingestion_into_data_lake

## Azure Storage Explorer
* datalake -> 


## Azure Dev
* Repos
* ENSI-BIG DATA

* ENSI-BIGDATABRICK
```
> KEYRUS
  > data_tranforms
    > raw
      > usr
        > ibge
          > org_raw_deflator_pnad.py
        > me
          > org_raw_exp_completa.py
> STI_INTELIGENCIA          
  > pipelines..
```
* ENSI-BIGDATADOCS
* ENSI-BIGDATAPRM

## Azure DATA FACTORY
```
> Data Factory
  > "PEN" Icon
    > Create a new branch <Branch name* "uniepro_feature_uld_pintec">
    > Base on* <master branch>
      > Pipeline    
      
> raw 
  > usr 
    > ibge 
      > org_raw_deflator
    > me
      > org_raw_exp_completa
      
> Properties
  > General
    > Name* org_raw_exp_completa

> Parameters
> databricks {"notebook":"me/org_raw_exp_completa"}
> files ["{'namespace':'me','file_folder':'exp_completa','extension':'csv','column_delimiter':';','encoding':'UTF-8','null_value':''}"]
> env {"env":"dev"}
        
```
 
 
## Azure DataBricks
  > Repos
    > ENSI-BIGDATABRICKS
      > uniepro_feature_uld > KEYRUS > data_transforms > raw > usr > ibge > CREATING THE FILES
      >  

